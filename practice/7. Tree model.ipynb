{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 의사결정나무 Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X가 가질 수 있는 값의 공간을 분할해서, 분할한 공간에 Y의 평균값을 하나의 예측값으로 할당한다.\n",
    "\n",
    "> 파티션이 너무 작을 경우, bias는 작아지겠으나 예측에의 분산이 매우 커진다.(Overfitting)\n",
    ">\n",
    "> 적당한 trade-off가 필요하다.\n",
    "\n",
    "`-` 논점\n",
    "\n",
    "1. 얼마나 잘게 쪼갤 것인가?\n",
    "2. 예측의 러프함을 어떻게 해결할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 나무 모형은 예측변수들의 공간(예측변수가 가질 수 있는 값들의 집합)에 대한 층화적 분할에 근거한다.\n",
    "\n",
    "> 유한한 공간.\n",
    "\n",
    "* 주어진 관측치에서의 예측값은 해당 관측치가 속한 예측변수의 영역(분할된 공간)에서의 반응변수들의 평균값으로 주어진다.(분류 문제도 마찬가지)\n",
    "\n",
    "* 간단하고 해석에서 장점이 있다.\n",
    "\n",
    "> 어떤 노드를 통과해야 해당 값으로 귀결되는가? -> 분류 등에서 체크하기 쉬움.\n",
    "\n",
    "* 다른 기계학습 방법들에 비해 좋은 성능을 보이지 못하는 경우가 많다.\n",
    "\n",
    "> 급격한 변화를 보이는 상황에서 예측오차가 커짐\n",
    ">\n",
    "> 새로운 자료의 투입에 민감함, 로버스트하지 못한 모형임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. 공간 분할의 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분할된 공간 혹은 나무에서의 최하단 부분을 terminal nodes / leaves라 한다.\n",
    "\n",
    "* Binary Splitting : 전체 공간을 하나의 예측변수를 기준으로 두 공간으로 분할한다.\n",
    "\n",
    "> 단, 하위 분할시에는 전체 공간이 아니라 이미 분할된 공간에 Binary Splitting을 적용함(층화적 분할)\n",
    "\n",
    "* 적절한 Stopping Rule에 따라 분할을 중지한다.\n",
    "\n",
    "`-` 가장 큰 제곱합의 감소를 가져다주는 변수와 분기점을 선택\n",
    "\n",
    "$$\\underset{i\\in R}{\\sum}(y_i - \\bar y)^2 - \\underset{k=1}{\\overset{2}{\\sum}}\\underset{i\\in R_k}{\\sum}(y_i - \\bar y_k)^2$$\n",
    "\n",
    "&nbsp; 전체의 오차제곱합과 노드 별 오차제곱합의 합. 해당 차이를 크게 만들어야 함.\n",
    "\n",
    "$$RSS = \\underset{j=1}{\\overset{J}{\\sum}}\\underset{i \\in R_j}{\\sum}(y_i - \\hat y_{R_j})^2$$\n",
    "\n",
    "&nbsp; 위를 최소화하는 예측변수의 값들을 선택하는 것을 목표로 함.\n",
    "\n",
    "> $\\hat y_{R_j}$는 $R_j$에서의 예측변수 평균, $R_j$는 terminal nodes의 영역들.\n",
    ">\n",
    "> 각각의 노드의 값은 노드 평균에 가까워지도록 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 나무모형은 예측성능은 좋을 수 있지만, 과적합 문제가 발생할 수 있음.\n",
    "\n",
    "> 리프의 샘플이 너무 작은 상황이 발생하면 분산이 커짐 -> 최소한 각각의 리프에 몇 개 이상의 데이터가 있어야 한다고 제약 걸 수 있음\n",
    "\n",
    "* 해석은 명확하나, 예측력의 관점에서는 그닥 좋은 것은 아님.\n",
    "\n",
    "* 나무의 크기는 모형의 복잡도와 연결된다. -> 적절히 모형을 단순화시켜줄 필요가 있다.\n",
    "\n",
    "`-` 해결방법\n",
    "\n",
    "> RSS의 감소가 특정 수준 이하로 떨어지면 중단하는 방법 : 이후 큰 RSS 감소를 불러일으키는 분할을 무시하게 될 수 있음.\n",
    ">\n",
    "> 나무를 매우 크게 키운 뒤 적절히 가지치기하여 단순화시키는 방법 : test error를 작게 하는 subtree를 찾음. 더 선호되는 방법.\n",
    "\n",
    "`-` Cost complexity Pruning\n",
    "\n",
    "* $\\alpha$의 적절한 그리드를 선택하고, 각 $\\alpha$에 대하여 다음을 최소화시키는 나무를 찾는다.\n",
    "\n",
    "$$\\underset{m=1}{\\overset{|T|}{\\sum}}\\underset{x_i \\in R_m}{\\sum}(y_i - \\hat y_{R_m}) + \\alpha|T|$$\n",
    "\n",
    "$|T| : $ Number of terminal node\n",
    "\n",
    "> 리프 노드를 T개로 증가시켰을 때 RSS 감소분이 모형의 복잡성 증가분보다 커야 한다.\n",
    ">\n",
    "> 선택된 $\\alpha$에 대응되는 나무가 최종 선택되는 모형을 선택한다.\n",
    ">\n",
    "> 노드의 개수를 고려하여 최적화 함수를 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "쪼갤 수 있는 모든 노드를 쪼갠 후, 병합하는 식으로 가지치기를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Classification trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률을 예측하고, 그 다음 범주를 예측함.\n",
    "\n",
    "빈도의 이질성을 중시\n",
    "\n",
    "> 얼마나 노드 안에 같은 class가 많이 있는지를 중요시한다. MSE와 다른 것을 사용하여 공간을 분할한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Measures for splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 오분류율 Classification error rate\n",
    "\n",
    "$$E = 1 - \\underset{k}{\\max}(\\hat p _{mk})$$\n",
    "\n",
    "> $\\hat p _{mk}$는 m노드에서 k범주의 비율\n",
    ">\n",
    "> 해당 값을 줄이는 것을 목표로 함. 한 노드에 같은 범주의 값만 존재한다면 0이 됨.\n",
    ">\n",
    "> 최대값에만 의존하며, 민감한 측도가 아님.\n",
    "\n",
    "* Gini index\n",
    "\n",
    "$$G = \\underset{k=1}{\\overset{K}{\\sum}}\\hat p_{mk}(1-\\hat p_{mk})$$\n",
    "\n",
    "> 해당 값이 0이 되려면, 한 노드에 같은 범주의 값만 존재해야 함.\n",
    "\n",
    "* Entropy\n",
    "\n",
    "$$D = - \\underset{k=1}{\\overset{K}{\\sum}} \\hat p_{mk} \\log \\hat p_{mk}$$\n",
    "\n",
    "> 해당 값이 0이 되려면, 확률이 0/1이여야 함.\n",
    ">\n",
    "> 해당 측도를 가장 많이 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini index와 Entropy가 주로 사용된다 : 하나의 노드가 두 개의 노드로 분할될 때, 가장 Gini index / Entropy가 많이 줄어드는 경우를 선택.\n",
    "\n",
    "> 해당 공간의 순도가 높을수록 0에 가까운 값. 즉, 불순도를 최소화시키는 방향으로 분할이 이뤄짐."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F. 트리 / 선형 모형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 각각 예측\n",
    "\n",
    "* Linear model\n",
    "\n",
    "$$f(X) = \\beta_0 + \\sum X_j \\beta_j$$\n",
    "\n",
    "* Regression trees\n",
    "\n",
    "$$f(X) = \\sum c_m I(X \\in R_m)$$\n",
    "\n",
    "$R_m$은 예측기의 공간\n",
    "\n",
    "> 실제 관계가 선형이 아니면, 나무 모형이 선형 모형보다 우수하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 나무 모형의 장단점\n",
    "\n",
    "* 장점\n",
    "\n",
    "> 모형에 대한 해석과 설명이 매우 직관적이고 간단\n",
    ">\n",
    "> 시각적으로 표현이 용이\n",
    ">\n",
    "> 예측변수에 범주형 변수가 있을 때, 가변수 처리가 필요하지 않음\n",
    "\n",
    "* 단점\n",
    "\n",
    "> 예측성능이 좋지 못한 경우가 많음\n",
    ">\n",
    "> 모형의 분산이 커서 robust하기 않은 경우가 많음. **데이터가 조금만 바뀌어도 생성되는 모형이 크게 변할 수 있음.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **G. Ensemble methods**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bagging : Bootstrap aggregation**. bootstrap의 통합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 만약 모집단으로부터 여러 개의 훈련자료를 얻을 수 있고, 이로부터 여러 개의 모형을 얻을 수 있다면, 이로부터 주어지는 average prediction에 의해 모형의 분산을 줄일 수 있다.\n",
    "\n",
    "* 보통은 한 set의 자료만이 주어지게 되므로, 대신 Bootstrap을 이용하여 한 set의 자료로부터 여러 개의 훈련자료를 생성해 낼 수 있다.\n",
    "\n",
    "$X_1, \\cdots, X_n$이 어떤 모집단으로부터의 i.i.d random sample이라 할 때, 이로부터의 복원추출로 같은 size의 표본을 여러 개 만들어낼 수 있다.\n",
    "\n",
    "위 과정을 총 $B$번 반복하면 총 $B$개의 bootstrap sample이 얻어지고, 각 sample을 훈련자료로 하여 $\\hat f^{*, 1}, \\cdots, \\hat f^{*, B}$개의 함수를 얻으면 다음과 같이 최종 예측함수를 구성할 수 있다.\n",
    "\n",
    "$$\\hat f_{\\text{bag}} (x) = \\frac1B \\sum \\hat f^{*, b}(x)$$\n",
    "\n",
    "$B$를 선택하는 것에서 문제가 있다. 많이 쓸수록 좋으나, 리소스 문제 때문에 적당한 개수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Bagging의 효과\n",
    "\n",
    "* 각 나무들을 prunning하지 않아도 된다. 즉, 각 나무들의 분산이 커지더라도 편의를 작게 가지도록 한다. -> 최적의 $\\alpha$따위는 선택하지 않고, 그냥 한다.\n",
    "\n",
    "* 여러 개의 샘플링을 할 경우, 노드를 매우 작게 만든다.\n",
    "\n",
    "* Bagging은 여러 회귀방법들에서 예측력의 향상을 가져올 수 있으나, 특히 나무 모형에서 효용이 매우 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Bagging for classification\n",
    "\n",
    "* 분류 문제에서는 예측값이 class로 주어지므로, 이를 평균내는 것은 의미가 없다.\n",
    "* Majority voting : B개의 개별 나무들의 예측값 중 가장 다수를 차지하는 class로 예측값을 할당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Out-of-Bag error estimation\n",
    "\n",
    "* Bagging 수행 시 특정 나무의 적합에 사용되지 않는 관측지들이 평균적으로 전체의 약 1/3정도 됨.\n",
    "\n",
    "> i번째 관측치에 대한 test error를 추정하고자 할 때, $B$개의 나무모형 중 i번째 관측치를 적합에 사용하지 않은 약 $B/3$개의 나무들에 의한 에측치를 이용할 수 있음.\n",
    "\n",
    "* 모든 관측치에 대하여 위와 같은 과정을 수행하여 최종 test error를 추정함.\n",
    "\n",
    "> CV 등, 계산이 많이 필요한 절차를 피할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Variable importance measure\n",
    "\n",
    "* 다른 형태의 여러 나무를 결합하면, 모형을 해석하는 것은 거의 불가능\n",
    "* 나무들을 생성할 때, 어떤 변수들이 RSS / Gini index 등의 큰 감소를 가져왔는지를 요약한 값으로 변수의 중요도 판단 가능\n",
    "\n",
    "> 해석을 하기 위한 도구에 불과. 완벽한 해석의 지표는 아님.\n",
    "\n",
    "* 나머지 변수를 병합하고, 특정 변수(수치형)가 증가할 때, 반응변수의 평균이 어떻게 바뀌는지를 파악하는 방법으로 해석할 수도 있음 -> PDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
