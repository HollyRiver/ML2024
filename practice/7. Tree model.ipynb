{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 의사결정나무 Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X가 가질 수 있는 값의 공간을 분할해서, 분할한 공간에 Y의 평균값을 하나의 예측값으로 할당한다.\n",
    "\n",
    "> 파티션이 너무 작을 경우, bias는 작아지겠으나 예측에의 분산이 매우 커진다.(Overfitting)\n",
    ">\n",
    "> 적당한 trade-off가 필요하다.\n",
    "\n",
    "`-` 논점\n",
    "\n",
    "1. 얼마나 잘게 쪼갤 것인가?\n",
    "2. 예측의 러프함을 어떻게 해결할 것인가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 나무 모형은 예측변수들의 공간(예측변수가 가질 수 있는 값들의 집합)에 대한 층화적 분할에 근거한다.\n",
    "\n",
    "> 유한한 공간.\n",
    "\n",
    "* 주어진 관측치에서의 예측값은 해당 관측치가 속한 예측변수의 영역(분할된 공간)에서의 반응변수들의 평균값으로 주어진다.(분류 문제도 마찬가지)\n",
    "\n",
    "* 간단하고 해석에서 장점이 있다.\n",
    "\n",
    "> 어떤 노드를 통과해야 해당 값으로 귀결되는가? -> 분류 등에서 체크하기 쉬움.\n",
    "\n",
    "* 다른 기계학습 방법들에 비해 좋은 성능을 보이지 못하는 경우가 많다.\n",
    "\n",
    "> 급격한 변화를 보이는 상황에서 예측오차가 커짐\n",
    ">\n",
    "> 새로운 자료의 투입에 민감함, 로버스트하지 못한 모형임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. 공간 분할의 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 분할된 공간 혹은 나무에서의 최하단 부분을 terminal nodes / leaves라 한다.\n",
    "\n",
    "* Binary Splitting : 전체 공간을 하나의 예측변수를 기준으로 두 공간으로 분할한다.\n",
    "\n",
    "> 단, 하위 분할시에는 전체 공간이 아니라 이미 분할된 공간에 Binary Splitting을 적용함(층화적 분할)\n",
    "\n",
    "* 적절한 Stopping Rule에 따라 분할을 중지한다.\n",
    "\n",
    "`-` 가장 큰 제곱합의 감소를 가져다주는 변수와 분기점을 선택\n",
    "\n",
    "$$\\underset{i\\in R}{\\sum}(y_i - \\bar y)^2 - \\underset{k=1}{\\overset{2}{\\sum}}\\underset{i\\in R_k}{\\sum}(y_i - \\bar y_k)^2$$\n",
    "\n",
    "&nbsp; 해당 차이를 크게 만들어야 함.\n",
    "\n",
    "$$RSS = \\underset{j=1}{\\overset{J}{\\sum}}\\underset{i \\in R_j}{\\sum}(y_i - \\hat y_{R_j})^2$$\n",
    "\n",
    "&nbsp; 위를 최소화하는 예측변수의 값들을 선택하는 것을 목표로 함.\n",
    "\n",
    "> $\\hat y_{R_j}$는 $R_j$에서의 예측변수 평균, $R_j$는 terminal nodes의 영역들."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 나무모형은 예측성능은 좋을 수 있지만, 과적합 문제가 발생할 수 있음.\n",
    "\n",
    "> 리프의 샘플이 너무 작은 상황이 발생하면 분산이 커짐 -> 최소한 각각의 리프에 몇 개 이상의 데이터가 있어야 한다고 제약 걸 수 있음\n",
    "\n",
    "* 나무의 크기는 모형의 복잡도와 연결된다. -> 적절히 모형을 단순화시켜줄 필요가 있다.\n",
    "\n",
    "`-` 해결방법\n",
    "\n",
    "> RSS의 감소가 특정 수준 이하로 떨어지면 중단하는 방법 : 이후 큰 RSS 감소를 불러일으키는 분할을 무시하게 될 수 있음.\n",
    ">\n",
    "> 나무를 매우 크게 키운 뒤 적절히 가지치기하여 단순화시키는 방법 : test error를 작게 하는 subtree를 찾음. 더 선호되는 방법.\n",
    "\n",
    "`-` Cost complexity Pruning\n",
    "\n",
    "* $\\alpha$의 적절한 그리드를 선택하고, 각 $\\alpha$에 대하여 다음을 최소화시키는 나무를 찾는다.\n",
    "\n",
    "$$\\underset{m=1}{\\overset{|T|}{\\sum}}\\underset{x_i \\in R_m}{\\sum}(y_i - \\hat y_{R_m}) + \\alpha|T|$$\n",
    "\n",
    "$|T| : $ Number of terminal node\n",
    "\n",
    "> 리프 노드를 T개로 증가시켰을 때 RSS 감소분이 모형의 복잡성 증가분보다 커야 한다.\n",
    ">\n",
    "> 선택된 $\\alpha$에 대응되는 나무가 최종 선택되는 모형을 선택한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
