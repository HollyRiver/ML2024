{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving beyond linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **A. 선형 모형의 한계**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 선형모형은 해석 및 적용이 단순하다는 장점이 있다.\n",
    "* 하지만, 모형이 제한적이므로 예측 성능에 한계가 존재하는 경우가 많다.\n",
    "\n",
    "`-` Polynomial regression\n",
    "\n",
    "> 적절한차수 p의 선택을 위하여 CV를 해야 함.\n",
    ">\n",
    "> 전역에서 곡률이 유사하면 문제가 없으나, 로컬에서 곡률이 바뀌면 적절하지 못함.\n",
    "\n",
    "---\n",
    "\n",
    "**Basis Function approach**\n",
    "\n",
    "`-` Regression splines\n",
    "\n",
    "> 예측변수의 공간을 K개로 나누어 각각 다항회귀모형을 적합한다. 적합된 함수들이 어느 정도의 smoothness를 가지도록 제한을 둔다.(미분을 몇 번 까지 할 수 있는지에 대한 제약. 2차, 3차 등)\n",
    ">\n",
    "> 구간을 잘게 쪼개는 편\n",
    ">\n",
    "> smoothness를 위해 구간을 overlaping하는 기법도 존재한다. (차수가 높아질 수록 많이 겹치게 만듦)\n",
    "\n",
    "$$f(x) = \\underset{k=1}{\\overset{K}{\\sum}} \\alpha_k B_k(x)$$\n",
    "\n",
    "* **구간(dots)을 얼마나 잡을지(데이터의 밀도가 높으면 dots을 더 촘촘하게... 보통 log(n)개로 잡는 것을 추천)** : CV를 쓰기도 함\n",
    "* dots의 위치를 어떻게 잡아야 하는지 : 이쪽은 CV를 쓰기 어려움. 일반적으로 uniform을 이용(equality)하거나, 데이터 포인트의 개수가 동일하도록 나누거나... > 이 정도 두 방식은 CV할 수 있겠네. 두배로 잡으면 되니까\n",
    "* 차수를 어떻게 잡을지(보통은 2차, 3차 정도 잡으면 잘 품)\n",
    "* Local basis의 선택이 중요\n",
    "\n",
    "`-` Smoothing splines\n",
    "\n",
    "> Regression splines와 비슷하나, shirinkage 방법을 동원.\n",
    "\n",
    "$$\\sum(y_i - y(x_i))^2 + \\lambda \\int (g''(t))^2 dt$$\n",
    "\n",
    "> 결국엔 함수의 변화가 급격하지 않도록 하는 것.\n",
    ">\n",
    "> 후항을 0으로 만들기 위해서는 $g$가 직선이여야 함. 이에 따라 함수를 추정할 때 직선으로 축소시키는 것에 중요시함.\n",
    ">\n",
    "> 변수 선택보다, Shirinkage가 더 편하거덩요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Regression splines**\n",
    "\n",
    "예측변수의 범위 바깥에서는 매우 큰 분산을 가지는 것으로 알려진다. 이를 개선하기 위해 최외곽 knot이 바깥에서는 선형 함수가 되도록 제한조건(natural splines)\n",
    "\n",
    "Natural cubic spline은 지엽적인 변화를 더 잘 감지할 수 있다.\n",
    "\n",
    "**Smoothing spline**\n",
    "\n",
    "$$\\sum(y_i - y(x_i))^2 + \\lambda \\int (g''(t))^2 dt$$\n",
    "\n",
    "> 목적함수의 후항을 0으로 만들기 위해서는(이계도함수가 0) 직선의 형태를 가지고 있어야 한다.\n",
    ">\n",
    "> 적절한 $lambda$를 사용하여 모형을 추정한다.\n",
    ">\n",
    "> 구간을 선정할 필요가 없어진다.\n",
    "\n",
    "Basis를 사용하는 경우, 예측은 ${\\bf S}_\\lambda y$가 된다. ${\\bf S}_\\lambda D_x(D_x^{\\top} + \\lambda \\Omega)^{-1}D_x$는 각 Basis와 추정된 계수에 의해서 결정된다.\n",
    "\n",
    "자유도는 $\\text{Tr}({\\bf S}_\\lambda)$이고, $\\lambda$가 작아질수록 함수의 자유도는 커질 것이다.\n",
    "\n",
    "> $\\lambda$를 결정함에 있어 자유도를 사용할 수 있다. 자유도의 선택 방법으로 CV를 사용한다. 일반적으로 자유도로 CV를 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다항회귀 : 지엽적인 곡률의 변화를 잘 잡지 못할 수 있음.\n",
    "\n",
    "Spline : 자료를 여러 개의 구간으로 나눠서 각각 모형을 구성함.\n",
    "\n",
    "> Smoothing spline : $\\lambda$를 선택, 자유도를 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Local (linear) Regression\n",
    "\n",
    "$$\\underset{i=1}{\\overset{n}{\\sum}} K_{i0}(x_i, x_0)(y_i - \\beta_0 - \\beta_1 x_i)^2$$\n",
    "\n",
    "> $x_0$에서 적합된 함수값은 $\\hat f(x_0) = \\beta_0 + \\beta_1 x_0$\n",
    ">\n",
    "> $K_{i0}(x_i,  x_0)$는 $x_0$인근에 대한 가중치를 부여하는 함수\n",
    ">\n",
    "> 성능과 특성이 좋으나, 계산량이 매우 많기 때문에 기계학습에서 적극적으로 사용되진 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Generalized additive models (GAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형모형에서 각 예측변수에 해당하는 선형항을 일반적인 비선형항으로 치환\n",
    "\n",
    "$$y_i = \\beta_0 + f_1(x_{i1}) + \\cdots + f_p(x_{ip}) + \\epsilon_i$$\n",
    "\n",
    "$y_i = \\beta_0 + f(x_{i1}, \\cdots, x_{ip})$로 보는 것이 아닌, 교호작용을 제거한 형태로 보는 것?\n",
    "\n",
    "> 원래의 $f$가 엄청나게 복잡하다는 가정을 조금 누르는 것.\n",
    ">\n",
    "> 특정 변수 간 interaction이 있을 경우 해당 항을 추가하기도 함. $f(x_3, x_4)$등\n",
    "\n",
    "* $E(y|x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 \\Rightarrow f_1(x_1) + f_2(x_2)$\n",
    "\n",
    "> $x_1$이 고정되어 있을 때 조건부 평균의 변화 ~~\n",
    ">\n",
    "> PDP를 볼 수도 있음. 변수 간 상호작용을 도시화 -> 너무 오래 된 거라서 다른 거 쓰자고 하기도 함.\n",
    "\n",
    "\n",
    "`-` 장점\n",
    "\n",
    "* 예측변수의 비선형 효과에 대한 모형화가 가능\n",
    "* Additive 구조로 인하여 각 $X_j$가 $Y$에 주는 효과를 개별적으로 해석 가능 -> DNN의 경우 타 변수들과 완전히 결합되어 있기 때문에 설명이 어려움.\n",
    "\n",
    "`-` 단점\n",
    "\n",
    "* interaction이 모형에 포함되지 않음 -> 상호작용을 새로운 예측변수로 GAM에 넣어 해결 가능하긴 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ex) $Y$가 binary일 때, 로지스틱 GAM\n",
    "\n",
    "$$log(\\frac{p(x)}{1-p(x)}) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p)$$\n",
    "\n",
    "$\\beta_0 + f_1(x_1) + f_2(x_2) -> \\alpha_1 + f_{11}(x_1), ~ \\alpha_2 + f_{22}(x_2) : $ 계수 합, 평균적인 수준은 정해지지만 계수의 분할이 유일하지 않음.\n",
    "\n",
    "> $f_1(x_1), ~ f_2(x_2)$의 평균을 0이라고 가정하여, 전체 평균을 $\\beta_0$로 제약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN은 설명변수들의 결합분포 자체를 추정함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
