{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 이론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` 기하학 기반의 분류 모형.\n",
    "\n",
    "* 확률적 모형이 아니여도 문제를 풀 수 있다라는 뭐시기. 나중엔 이것도 확률과 뭐시기...\n",
    "\n",
    "> 확률이라는 게 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 초평면"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 그룹을 나누는 선을 정하는 문제.\n",
    "* 선 위에 있는지, 선 가운데에 있는지(분류 못함), 선 아래에 있는지.\n",
    "\n",
    "* decision boundary를 구할 때, 구분선과 개별 점 간의 거리를 계산하여 각 점과의 거리 중 가장 작은 것을 margin으로 정함.\n",
    "\n",
    "> margin을 가장 크게 하는 구분선을 설정함. 이 때, 모든 데이터 포인트가 필요하지 않고 경계에 가까이 있는 데이터만 사용되게 됨.\n",
    ">\n",
    "> margin을 구할 때 사용하는 데이터 포인트들을 support vector라고 함.\n",
    "\n",
    "* 실제로 사용하는 데이터 포인트들은 상당히 적음. 데이터의 크기가 상당히 커도 계산량이 적음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. support vector인지 식별하는 알고리즘 적용\n",
    "2. margin을 계산하여 이를 가장 크게 하는 decision boundary를 식별"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-` Real data\n",
    "\n",
    "* 각 그룹이 완벽하게 나뉘지 않고 섞여있을 수가 있음.\n",
    "\n",
    "해결법\n",
    "> embedding : 데이터를 어떻게 변환을 하여 decision boundary가 식별 될 수 있도록 만듦. -> kernel 응용.\n",
    ">\n",
    "> soft margin : margin을 maximum으로 만들되, 애초에 데이터가 margin 안으로 들어갈 수 있도록 허용을 하겠다. 넘나드는 것을 허용하겠다. -> 가중치를 하이퍼파라미터로 먹어서 하면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리\n",
    "\n",
    "* 두 개의 레이블이 다른 데이터의 사이(거리)가 벌어져있는 경우를 생각하자. 이 때, 두 개의 데이터를 나눌 때 margin을 최대로 하는 decision boundary를 찾는다.\n",
    "\n",
    "* 두 레이블이 완벽히 나뉘지 않을 땐, 데이터를 임베딩하거나 margin 안쪽으로 데이터가 넘나드는 것을 일부 허용하여 문제를 풀이한다.\n",
    "\n",
    "* 의미 있는 데이터 : 서포트 벡터만 가지괴 모형을 구성하다보니, 속도가 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이론 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "초평면 위에 있다\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_{i1} + \\cdots \\beta_p x_{ip} > 0 ~ \\text{if}~ y_i = 1$$\n",
    "\n",
    "초평면 아래에 있다\n",
    "\n",
    "$$\\beta_0 + \\beta_1 x_{i1} + \\cdots \\beta_p x_{ip} < 0 ~ \\text{if}~ y_i = -1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Seperating hyperplane은 무한이 많이 존재한다. 하지만 관측치들이 가능하면 decision boundary로부터 멀리 떨어지도록 하는 것이 좋다.\n",
    "\n",
    "* 훈련자료와 decision boundary의 최소 거리를 margin이라 한다.\n",
    "\n",
    "* 잘 분류가 되었다면 test data에서의 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* 기본적으로 margin을 maximize 한다.\n",
    "* margin을 계산할 필요가 없는 포인트들이 존재하여 해당 포인트는 고려하지 않는다.\n",
    "* 새로운 데이터가 들어왔을 때 예측을 제대로 수행하기 위하여 decision boundary를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximal margin classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{maximize} ~ M$$\n",
    "\n",
    "$$\\sum \\beta_j^2 = 1$$\n",
    "\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) > M$$\n",
    "\n",
    "> 잘 분류되었을 때 두 값의 부호가 같으므로, 무조건 양수가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{max}\\{\\frac12 ||\\beta||^2 - \\sum \\alpha_i (y_i(x_i^{\\top}\\beta + \\beta_0)-1)\\}$$\n",
    "\n",
    "> $\\alpha = 0$이면 해당 데이터포인트는 사용하지 않는 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 실제로는 관측치가 두 class로 정확히 구분되지 않는 경우가 더 많다.\n",
    "\n",
    "* soft margin : margin 안쪽으로 들어가는 것을 허용, 오분류 하는 것까지 허용.\n",
    "\n",
    "\n",
    "$$y_i(\\beta_0 + \\beta_1 x_{i1} + \\cdots + \\beta_p x_{ip}) > M(1-\\epsilon_i)$$\n",
    "\n",
    "$$\\epsilon_i > 0, \\sum \\epsilon_i ≤ C$$\n",
    "\n",
    "> $C$ : tuning parameter. 양수, margin을 얼마나 넘나들게 할 것인지. $\\epsilon_i$ : slack variable.\n",
    "\n",
    "\n",
    "* $\\epsilon_i > 0$이면, margin의 안쪽에 위치하게 되며, $\\epsilon_i > 1$이면 오분류된다.\n",
    "\n",
    "* $C$가 커질수록, margin이 넓어지게 된다. margin의 의미가 약해진다.\n",
    "\n",
    "* $C$를 작게 하면, 현 데이터의 정확한 분류에 더 집중하게 되므로, 자료 적합성이 향상한다. -> bias는 감소하고 variance는 증가한다. overfitting\n",
    "\n",
    "* margin 바깥에 위치한 관측치들은 classifier의 결정에 아무런 영향도 주지 않는다. -> Robust to outlier\n",
    "\n",
    "* margin 위, 혹은 안쪽에 위치한 관측치들을 support vector라고 한다. $C$가 클수록 support vector가 많아진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-linear decision boundary\n",
    "\n",
    "* 실제 decision boundary가 비선형일 때, 초평면만 고려하면 좋은 성능을 보이지 못한다.\n",
    "* 고차원의 decision boundary를 고려해서 개선할 수 있다.\n",
    "\n",
    "Support Vector classifier는 다음과 같이 선형함수 형태로 표현됨이 알려져 있다.\n",
    "\n",
    "\n",
    "$$f(x) = \\beta_0 + \\sum \\alpha_i <x, x_i> : <,> = \\text{inner product}$$\n",
    "\n",
    "\n",
    "`-` Kernel 함수 : 일반화된 내적\n",
    "\n",
    "$$f(x) = \\beta_0 + \\sum \\alpha_i K(x, x_i)$$\n",
    "\n",
    "* Linear kernel : hyperplane과 동일\n",
    "\n",
    "$$K(x_i, x_i') = \\sum x_{ij} x_{i'j}$$\n",
    "\n",
    "* Polynomial kernel with order $d$\n",
    "\n",
    "$$K(x_i, x_i') = (1+\\sum x_{ij}x_{i'j})$$\n",
    "\n",
    "* Radial Kernel : Gaussian Kernel. 일반적으로 제일 powerful\n",
    "\n",
    "$$K(x_i, x_i') = \\exp(-\\gamma \\sum (x_{ij} - x_{i'j})^2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률값이 없는데 어떻게 1-spe, sen을 구하나?\n",
    "\n",
    "> decision boundary와의 거리 정보만 구함. 양수면 위로 많이 떨어져있고, 음수면 아래로 많이 떨어져 있음.\n",
    ">\n",
    "> 거리 정보를 cut-off value로 하여 바꿔보고 ROC를 구할 수 있음.\n",
    ">\n",
    "> 확률값을 산출하는 다른 기계학습의 ROC와는 결이 다름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
